{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging using modified Viterbi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flowchart of the solution -\n",
    "\n",
    "<ul>\n",
    "    <li>EDA to understand training corpus.</li>\n",
    "    <li>Plain vanilla model building.</li>\n",
    "    <li>Test plain vanilla model on test set and understand the problem.</li>\n",
    "    <li>Refining viterbi model using other pos tagging technique</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "#Importing libraries\n",
    "import nltk, re, pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pprint, time\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the Treebank tagged sentences\n",
    "nltk_data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3718\n",
      "196\n"
     ]
    }
   ],
   "source": [
    "# Splitting into train and test\n",
    "# we will consider train : test ratio of 95:5 as suggested in assignment\n",
    "random.seed(1234)\n",
    "train_set, test_set = train_test_split(nltk_data,test_size=0.05)\n",
    "\n",
    "print(len(train_set))\n",
    "print(len(test_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95809"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting list of tagged words\n",
    "train_tagged_words = [tup for sent in train_set for tup in sent]\n",
    "len(train_tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no of tagged words available in dataset - 95790"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "{'NOUN', 'ADP', '.', 'DET', 'ADJ', 'PRON', 'NUM', 'PRT', 'VERB', 'X', 'CONJ', 'ADV'}\n"
     ]
    }
   ],
   "source": [
    "#Let's see how many unique tags we have in our dataset\n",
    "tags = [tup[1]  for sen in nltk_data for tup in sen]\n",
    "print(len(set(tags)))\n",
    "print(set(tags))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considering duplicate - 100676\n",
      "Unique words - 12408\n"
     ]
    }
   ],
   "source": [
    "#we can see that universal dataset has only 12 tags\n",
    "#Let's see how many unique word dataset has\n",
    "voc = [tup[0]  for sen in nltk_data for tup in sen]\n",
    "print(\"Considering duplicate -\",len(voc))\n",
    "#total no. words present in dataset (including duplicate) \n",
    "print(\"Unique words -\",len(set(voc)))\n",
    "#total no. of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = set(tags)\n",
    "voc = set(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NOUN', 'ADP', '.', 'DET', 'ADJ', 'PRON', 'NUM', 'PRT', 'VERB', 'X', 'CONJ', 'ADV'}\n"
     ]
    }
   ],
   "source": [
    "#print all the available tags\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this method we'll use to understand tags related with words\n",
    "def plot_cnt_words(word):\n",
    "    l_words = []\n",
    "    for tag in tags:\n",
    "        c = 0\n",
    "        for w,t in train_tagged_words:\n",
    "            if (w==word)&(t==tag):\n",
    "                    c += 1\n",
    "        if c > 0:\n",
    "            l_words.append((tag,c))\n",
    "    print(l_words)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NOUN', 2), ('ADP', 1), ('ADJ', 3), ('CONJ', 1423)]\n"
     ]
    }
   ],
   "source": [
    "#demo\n",
    "plot_cnt_words(\"and\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the vanilla Viterbi based POS tagger\n",
    "Let's build HMM viterbi model. (This model will act as a base model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first step is to find emission and transition probablities\n",
    "t = len(tags)\n",
    "v = len(voc)\n",
    "w_given_t = np.zeros((t, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 12408)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_given_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute word given tag: Emission Probability\n",
    "def word_given_tag(word, tag, train_bag = train_tagged_words):\n",
    "    tag_list = [pair for pair in train_bag if pair[1]==tag]\n",
    "    count_tag = len(tag_list)\n",
    "    w_given_tag_list = [pair[0] for pair in tag_list if pair[0]==word]\n",
    "    count_w_given_tag = len(w_given_tag_list)\n",
    "    \n",
    "    return (count_w_given_tag, count_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 6084)\n",
      "(52, 12904)\n",
      "(0, 27480) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's check w\n",
    "print(word_given_tag('and', 'ADJ'))\n",
    "print(word_given_tag('does', 'VERB'))\n",
    "print(word_given_tag('flight', 'NOUN'), \"\\n\")\n",
    "#flight word is not present in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute tag given tag: tag2(t2) given tag1 (t1), i.e. Transition Probability\n",
    "\n",
    "def t2_given_t1(t2, t1, train_bag = train_tagged_words):\n",
    "    tags = [pair[1] for pair in train_bag]\n",
    "    count_t1 = len([t for t in tags if t==t1])\n",
    "    count_t2_t1 = 0\n",
    "    for index in range(len(tags)-1):\n",
    "        if tags[index]==t1 and tags[index+1] == t2:\n",
    "            count_t2_t1 += 1\n",
    "    return (count_t2_t1, count_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(744, 2142)\n"
     ]
    }
   ],
   "source": [
    "# examples\n",
    "print(t2_given_t1(t2='NOUN', t1='CONJ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating t x t transition matrix of tags\n",
    "# each column is t2, each row is t1\n",
    "# thus M(i, j) represents P(tj given ti)\n",
    "\n",
    "tags_matrix = np.zeros((len(tags), len(tags)), dtype='float32')\n",
    "for i, t1 in enumerate(list(tags)):\n",
    "    for j, t2 in enumerate(list(tags)): \n",
    "        tags_matrix[i, j] = t2_given_t1(t2, t1)[0]/t2_given_t1(t2, t1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.64519662e-01, 1.76491991e-01, 2.40393013e-01, 1.32096075e-02,\n",
       "        1.21906837e-02, 4.54876292e-03, 9.60698724e-03, 4.37772907e-02,\n",
       "        1.47161573e-01, 2.92940326e-02, 4.19941768e-02, 1.68122277e-02],\n",
       "       [3.21725249e-01, 1.75718851e-02, 3.94036211e-02, 3.22151214e-01,\n",
       "        1.07774228e-01, 6.95420653e-02, 6.27263039e-02, 1.38445152e-03,\n",
       "        8.20021331e-03, 3.50372754e-02, 8.51970166e-04, 1.36315227e-02],\n",
       "       [2.22669885e-01, 9.13242027e-02, 9.39206704e-02, 1.72799706e-01,\n",
       "        4.38714288e-02, 6.50908798e-02, 8.13859776e-02, 2.14880472e-03,\n",
       "        8.86381939e-02, 2.70391256e-02, 5.91816641e-02, 5.18399142e-02],\n",
       "       [6.38747871e-01, 9.30626038e-03, 1.83707997e-02, 5.55958413e-03,\n",
       "        2.03770846e-01, 3.50495521e-03, 2.17548944e-02, 2.41721049e-04,\n",
       "        3.96422520e-02, 4.58061397e-02, 4.83442098e-04, 1.28112156e-02],\n",
       "       [7.01347768e-01, 7.79092684e-02, 6.34450987e-02, 4.93096653e-03,\n",
       "        6.60749525e-02, 6.57462224e-04, 2.05456931e-02, 1.10124918e-02,\n",
       "        1.24917813e-02, 2.05456931e-02, 1.61078237e-02, 4.93096653e-03],\n",
       "       [2.09132776e-01, 2.30237916e-02, 4.10590954e-02, 9.20951646e-03,\n",
       "        7.25249425e-02, 8.05832725e-03, 7.67459720e-03, 1.30468151e-02,\n",
       "        4.86953199e-01, 8.94090533e-02, 5.37221786e-03, 3.45356874e-02],\n",
       "       [3.53723407e-01, 3.33924368e-02, 1.17316782e-01, 3.25059099e-03,\n",
       "        3.22104022e-02, 1.47754140e-03, 1.86170205e-01, 2.74822693e-02,\n",
       "        1.77304968e-02, 2.11879432e-01, 1.27068562e-02, 2.65957438e-03],\n",
       "       [2.44618401e-01, 2.02217866e-02, 4.30528373e-02, 1.01435095e-01,\n",
       "        8.54533613e-02, 1.85909979e-02, 5.74037842e-02, 1.95694715e-03,\n",
       "        4.01500314e-01, 1.40247876e-02, 1.95694715e-03, 9.78473574e-03],\n",
       "       [1.10585868e-01, 9.17544961e-02, 3.47954109e-02, 1.33756980e-01,\n",
       "        6.57160580e-02, 3.54153737e-02, 2.26286426e-02, 3.16955969e-02,\n",
       "        1.68629885e-01, 2.17606947e-01, 5.34717925e-03, 8.20675790e-02],\n",
       "       [6.25893548e-02, 1.45512313e-01, 1.63463071e-01, 5.49642555e-02,\n",
       "        1.71564743e-02, 5.63939624e-02, 2.85941223e-03, 1.84432089e-01,\n",
       "        2.02859417e-01, 7.41858631e-02, 1.00079430e-02, 2.55758539e-02],\n",
       "       [3.47338945e-01, 5.46218492e-02, 3.64145674e-02, 1.20915033e-01,\n",
       "        1.19047619e-01, 5.92903830e-02, 4.06162478e-02, 4.66853427e-03,\n",
       "        1.54061630e-01, 8.40336177e-03, 4.66853409e-04, 5.41549958e-02],\n",
       "       [3.11774462e-02, 1.16749585e-01, 1.37313426e-01, 6.73300177e-02,\n",
       "        1.28689885e-01, 1.52570484e-02, 3.11774462e-02, 1.45936981e-02,\n",
       "        3.46931994e-01, 2.28855722e-02, 6.96517434e-03, 8.09286907e-02]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the matrix to a df for better readability\n",
    "tags_df = pd.DataFrame(tags_matrix, columns = list(tags), index=list(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NOUN</th>\n",
       "      <th>ADP</th>\n",
       "      <th>.</th>\n",
       "      <th>DET</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>PRON</th>\n",
       "      <th>NUM</th>\n",
       "      <th>PRT</th>\n",
       "      <th>VERB</th>\n",
       "      <th>X</th>\n",
       "      <th>CONJ</th>\n",
       "      <th>ADV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>0.264520</td>\n",
       "      <td>0.176492</td>\n",
       "      <td>0.240393</td>\n",
       "      <td>0.013210</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.004549</td>\n",
       "      <td>0.009607</td>\n",
       "      <td>0.043777</td>\n",
       "      <td>0.147162</td>\n",
       "      <td>0.029294</td>\n",
       "      <td>0.041994</td>\n",
       "      <td>0.016812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADP</th>\n",
       "      <td>0.321725</td>\n",
       "      <td>0.017572</td>\n",
       "      <td>0.039404</td>\n",
       "      <td>0.322151</td>\n",
       "      <td>0.107774</td>\n",
       "      <td>0.069542</td>\n",
       "      <td>0.062726</td>\n",
       "      <td>0.001384</td>\n",
       "      <td>0.008200</td>\n",
       "      <td>0.035037</td>\n",
       "      <td>0.000852</td>\n",
       "      <td>0.013632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.222670</td>\n",
       "      <td>0.091324</td>\n",
       "      <td>0.093921</td>\n",
       "      <td>0.172800</td>\n",
       "      <td>0.043871</td>\n",
       "      <td>0.065091</td>\n",
       "      <td>0.081386</td>\n",
       "      <td>0.002149</td>\n",
       "      <td>0.088638</td>\n",
       "      <td>0.027039</td>\n",
       "      <td>0.059182</td>\n",
       "      <td>0.051840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DET</th>\n",
       "      <td>0.638748</td>\n",
       "      <td>0.009306</td>\n",
       "      <td>0.018371</td>\n",
       "      <td>0.005560</td>\n",
       "      <td>0.203771</td>\n",
       "      <td>0.003505</td>\n",
       "      <td>0.021755</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.039642</td>\n",
       "      <td>0.045806</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>0.012811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADJ</th>\n",
       "      <td>0.701348</td>\n",
       "      <td>0.077909</td>\n",
       "      <td>0.063445</td>\n",
       "      <td>0.004931</td>\n",
       "      <td>0.066075</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>0.020546</td>\n",
       "      <td>0.011012</td>\n",
       "      <td>0.012492</td>\n",
       "      <td>0.020546</td>\n",
       "      <td>0.016108</td>\n",
       "      <td>0.004931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRON</th>\n",
       "      <td>0.209133</td>\n",
       "      <td>0.023024</td>\n",
       "      <td>0.041059</td>\n",
       "      <td>0.009210</td>\n",
       "      <td>0.072525</td>\n",
       "      <td>0.008058</td>\n",
       "      <td>0.007675</td>\n",
       "      <td>0.013047</td>\n",
       "      <td>0.486953</td>\n",
       "      <td>0.089409</td>\n",
       "      <td>0.005372</td>\n",
       "      <td>0.034536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>0.353723</td>\n",
       "      <td>0.033392</td>\n",
       "      <td>0.117317</td>\n",
       "      <td>0.003251</td>\n",
       "      <td>0.032210</td>\n",
       "      <td>0.001478</td>\n",
       "      <td>0.186170</td>\n",
       "      <td>0.027482</td>\n",
       "      <td>0.017730</td>\n",
       "      <td>0.211879</td>\n",
       "      <td>0.012707</td>\n",
       "      <td>0.002660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRT</th>\n",
       "      <td>0.244618</td>\n",
       "      <td>0.020222</td>\n",
       "      <td>0.043053</td>\n",
       "      <td>0.101435</td>\n",
       "      <td>0.085453</td>\n",
       "      <td>0.018591</td>\n",
       "      <td>0.057404</td>\n",
       "      <td>0.001957</td>\n",
       "      <td>0.401500</td>\n",
       "      <td>0.014025</td>\n",
       "      <td>0.001957</td>\n",
       "      <td>0.009785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>0.110586</td>\n",
       "      <td>0.091754</td>\n",
       "      <td>0.034795</td>\n",
       "      <td>0.133757</td>\n",
       "      <td>0.065716</td>\n",
       "      <td>0.035415</td>\n",
       "      <td>0.022629</td>\n",
       "      <td>0.031696</td>\n",
       "      <td>0.168630</td>\n",
       "      <td>0.217607</td>\n",
       "      <td>0.005347</td>\n",
       "      <td>0.082068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>0.062589</td>\n",
       "      <td>0.145512</td>\n",
       "      <td>0.163463</td>\n",
       "      <td>0.054964</td>\n",
       "      <td>0.017156</td>\n",
       "      <td>0.056394</td>\n",
       "      <td>0.002859</td>\n",
       "      <td>0.184432</td>\n",
       "      <td>0.202859</td>\n",
       "      <td>0.074186</td>\n",
       "      <td>0.010008</td>\n",
       "      <td>0.025576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONJ</th>\n",
       "      <td>0.347339</td>\n",
       "      <td>0.054622</td>\n",
       "      <td>0.036415</td>\n",
       "      <td>0.120915</td>\n",
       "      <td>0.119048</td>\n",
       "      <td>0.059290</td>\n",
       "      <td>0.040616</td>\n",
       "      <td>0.004669</td>\n",
       "      <td>0.154062</td>\n",
       "      <td>0.008403</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.054155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>0.031177</td>\n",
       "      <td>0.116750</td>\n",
       "      <td>0.137313</td>\n",
       "      <td>0.067330</td>\n",
       "      <td>0.128690</td>\n",
       "      <td>0.015257</td>\n",
       "      <td>0.031177</td>\n",
       "      <td>0.014594</td>\n",
       "      <td>0.346932</td>\n",
       "      <td>0.022886</td>\n",
       "      <td>0.006965</td>\n",
       "      <td>0.080929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          NOUN       ADP         .       DET       ADJ      PRON       NUM  \\\n",
       "NOUN  0.264520  0.176492  0.240393  0.013210  0.012191  0.004549  0.009607   \n",
       "ADP   0.321725  0.017572  0.039404  0.322151  0.107774  0.069542  0.062726   \n",
       ".     0.222670  0.091324  0.093921  0.172800  0.043871  0.065091  0.081386   \n",
       "DET   0.638748  0.009306  0.018371  0.005560  0.203771  0.003505  0.021755   \n",
       "ADJ   0.701348  0.077909  0.063445  0.004931  0.066075  0.000657  0.020546   \n",
       "PRON  0.209133  0.023024  0.041059  0.009210  0.072525  0.008058  0.007675   \n",
       "NUM   0.353723  0.033392  0.117317  0.003251  0.032210  0.001478  0.186170   \n",
       "PRT   0.244618  0.020222  0.043053  0.101435  0.085453  0.018591  0.057404   \n",
       "VERB  0.110586  0.091754  0.034795  0.133757  0.065716  0.035415  0.022629   \n",
       "X     0.062589  0.145512  0.163463  0.054964  0.017156  0.056394  0.002859   \n",
       "CONJ  0.347339  0.054622  0.036415  0.120915  0.119048  0.059290  0.040616   \n",
       "ADV   0.031177  0.116750  0.137313  0.067330  0.128690  0.015257  0.031177   \n",
       "\n",
       "           PRT      VERB         X      CONJ       ADV  \n",
       "NOUN  0.043777  0.147162  0.029294  0.041994  0.016812  \n",
       "ADP   0.001384  0.008200  0.035037  0.000852  0.013632  \n",
       ".     0.002149  0.088638  0.027039  0.059182  0.051840  \n",
       "DET   0.000242  0.039642  0.045806  0.000483  0.012811  \n",
       "ADJ   0.011012  0.012492  0.020546  0.016108  0.004931  \n",
       "PRON  0.013047  0.486953  0.089409  0.005372  0.034536  \n",
       "NUM   0.027482  0.017730  0.211879  0.012707  0.002660  \n",
       "PRT   0.001957  0.401500  0.014025  0.001957  0.009785  \n",
       "VERB  0.031696  0.168630  0.217607  0.005347  0.082068  \n",
       "X     0.184432  0.202859  0.074186  0.010008  0.025576  \n",
       "CONJ  0.004669  0.154062  0.008403  0.000467  0.054155  \n",
       "ADV   0.014594  0.346932  0.022886  0.006965  0.080929  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this dataframe is usefull to calculate tag probablities\n",
    "tags_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viterbi Algorithm - Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95809"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viterbi Heuristic\n",
    "def Viterbi(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "            state_probability = emission_p * transition_p    \n",
    "            p.append(state_probability)\n",
    "            \n",
    "        pmax = max(p)\n",
    "        # getting state for which probability is maximum\n",
    "        state_max = T[p.index(pmax)] \n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluating on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Profit', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('at', 'ADP'),\n",
       "  ('least', 'ADJ'),\n",
       "  ('in', 'ADP'),\n",
       "  ('the', 'DET'),\n",
       "  ('short', 'ADJ'),\n",
       "  ('term', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('is', 'VERB'),\n",
       "  ('usually', 'ADV'),\n",
       "  ('a', 'DET'),\n",
       "  ('secondary', 'ADJ'),\n",
       "  ('goal', 'NOUN'),\n",
       "  ('.', '.')],\n",
       " [('School', 'NOUN'),\n",
       "  ('officials', 'NOUN'),\n",
       "  ('and', 'CONJ'),\n",
       "  ('prosecutors', 'NOUN'),\n",
       "  ('say', 'VERB'),\n",
       "  ('0', 'X'),\n",
       "  ('Mrs.', 'NOUN'),\n",
       "  ('Yeargin', 'NOUN'),\n",
       "  ('is', 'VERB'),\n",
       "  ('lying', 'VERB'),\n",
       "  ('.', '.')],\n",
       " [('They', 'PRON'),\n",
       "  ('call', 'VERB'),\n",
       "  ('it', 'PRON'),\n",
       "  ('``', '.'),\n",
       "  ('photographic', 'ADJ'),\n",
       "  (\"''\", '.'),\n",
       "  ('.', '.')],\n",
       " [('Her', 'PRON'),\n",
       "  ('alternative', 'NOUN'),\n",
       "  ('was', 'VERB'),\n",
       "  ('90', 'NUM'),\n",
       "  ('days', 'NOUN'),\n",
       "  ('in', 'ADP'),\n",
       "  ('jail', 'NOUN'),\n",
       "  ('.', '.')],\n",
       " [('Under', 'ADP'),\n",
       "  ('a', 'DET'),\n",
       "  ('1934', 'NUM'),\n",
       "  ('law', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('the', 'DET'),\n",
       "  ('Johnson', 'NOUN'),\n",
       "  ('Debt', 'NOUN'),\n",
       "  ('Default', 'NOUN'),\n",
       "  ('Act', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('as', 'ADV'),\n",
       "  ('*', 'X'),\n",
       "  ('amended', 'VERB'),\n",
       "  ('*-1', 'X'),\n",
       "  (',', '.'),\n",
       "  ('it', 'PRON'),\n",
       "  ('*EXP*-2', 'X'),\n",
       "  (\"'s\", 'VERB'),\n",
       "  ('illegal', 'ADJ'),\n",
       "  ('for', 'ADP'),\n",
       "  ('Americans', 'NOUN'),\n",
       "  ('to', 'PRT'),\n",
       "  ('extend', 'VERB'),\n",
       "  ('credit', 'NOUN'),\n",
       "  ('to', 'PRT'),\n",
       "  ('countries', 'NOUN'),\n",
       "  ('in', 'ADP'),\n",
       "  ('default', 'NOUN'),\n",
       "  ('to', 'PRT'),\n",
       "  ('the', 'DET'),\n",
       "  ('U.S.', 'NOUN'),\n",
       "  ('government', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('unless', 'ADP'),\n",
       "  ('they', 'PRON'),\n",
       "  ('are', 'VERB'),\n",
       "  ('members', 'NOUN'),\n",
       "  ('of', 'ADP'),\n",
       "  ('the', 'DET'),\n",
       "  ('World', 'NOUN'),\n",
       "  ('Bank', 'NOUN'),\n",
       "  ('and', 'CONJ'),\n",
       "  ('International', 'NOUN'),\n",
       "  ('Monetary', 'NOUN'),\n",
       "  ('Fund', 'NOUN'),\n",
       "  ('.', '.')]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Running on entire test dataset would take more than 3-4hrs. \n",
    "# Let's test our Viterbi algorithm on a few sample sentences of test dataset\n",
    "\n",
    "random.seed(1234)\n",
    "\n",
    "# choose random 5 sents\n",
    "rndom = [random.randint(1,len(test_set)) for x in range(5)]\n",
    "\n",
    "# list of sents\n",
    "test_run = [test_set[i] for i in rndom]\n",
    "\n",
    "# list of tagged words\n",
    "test_run_base = [tup for sent in test_run for tup in sent]\n",
    "\n",
    "# list of untagged words\n",
    "test_tagged_words = [tup[0] for sent in test_run for tup in sent]\n",
    "test_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagging the test sentences\n",
    "start = time.time()\n",
    "tagged_seq = Viterbi(test_tagged_words)\n",
    "end = time.time()\n",
    "difference = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in seconds:  18.63800048828125\n",
      "[('Profit', 'NOUN'), (',', '.'), ('at', 'ADP'), ('least', 'ADJ'), ('in', 'ADP'), ('the', 'DET'), ('short', 'ADJ'), ('term', 'NOUN'), (',', '.'), ('is', 'VERB'), ('usually', 'ADV'), ('a', 'DET'), ('secondary', 'ADJ'), ('goal', 'NOUN'), ('.', '.'), ('School', 'NOUN'), ('officials', 'NOUN'), ('and', 'CONJ'), ('prosecutors', 'NOUN'), ('say', 'VERB'), ('0', 'X'), ('Mrs.', 'NOUN'), ('Yeargin', 'NOUN'), ('is', 'VERB'), ('lying', 'VERB'), ('.', '.'), ('They', 'PRON'), ('call', 'VERB'), ('it', 'PRON'), ('``', '.'), ('photographic', 'NOUN'), (\"''\", '.'), ('.', '.'), ('Her', 'PRON'), ('alternative', 'NOUN'), ('was', 'VERB'), ('90', 'NUM'), ('days', 'NOUN'), ('in', 'ADP'), ('jail', 'NOUN'), ('.', '.'), ('Under', 'ADP'), ('a', 'DET'), ('1934', 'NOUN'), ('law', 'NOUN'), (',', '.'), ('the', 'DET'), ('Johnson', 'NOUN'), ('Debt', 'NOUN'), ('Default', 'NOUN'), ('Act', 'NOUN'), (',', '.'), ('as', 'ADP'), ('*', 'X'), ('amended', 'NOUN'), ('*-1', 'X'), (',', '.'), ('it', 'PRON'), ('*EXP*-2', 'X'), (\"'s\", 'PRT'), ('illegal', 'ADJ'), ('for', 'ADP'), ('Americans', 'NOUN'), ('to', 'PRT'), ('extend', 'VERB'), ('credit', 'NOUN'), ('to', 'PRT'), ('countries', 'NOUN'), ('in', 'ADP'), ('default', 'NOUN'), ('to', 'PRT'), ('the', 'DET'), ('U.S.', 'NOUN'), ('government', 'NOUN'), (',', '.'), ('unless', 'ADP'), ('they', 'PRON'), ('are', 'VERB'), ('members', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('World', 'NOUN'), ('Bank', 'NOUN'), ('and', 'CONJ'), ('International', 'NOUN'), ('Monetary', 'NOUN'), ('Fund', 'NOUN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Time taken in seconds: \", difference)\n",
    "print(tagged_seq)\n",
    "#print(test_run_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "check = [i for i, j in zip(tagged_seq, test_run_base) if i == j] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = len(check)/len(tagged_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9431818181818182"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy\n",
    "#92#94.62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_tagged_cases = [[test_run_base[i-1],j] for i, j in enumerate(zip(tagged_seq, test_run_base)) if j[0]!=j[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('``', '.'), (('photographic', 'NOUN'), ('photographic', 'ADJ'))],\n",
       " [('a', 'DET'), (('1934', 'NOUN'), ('1934', 'NUM'))],\n",
       " [(',', '.'), (('as', 'ADP'), ('as', 'ADV'))],\n",
       " [('*', 'X'), (('amended', 'NOUN'), ('amended', 'VERB'))],\n",
       " [('*EXP*-2', 'X'), ((\"'s\", 'PRT'), (\"'s\", 'VERB'))]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_tagged_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's test this model on test sentences which contains words which are not present in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing\n",
    "\n",
    "def test_vertibi_simple(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    tagged_seq = Viterbi(words)\n",
    "    return tagged_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Android', 'NOUN'),\n",
       " ('has', 'VERB'),\n",
       " ('been', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('best-selling', 'ADJ'),\n",
       " ('OS', 'NOUN'),\n",
       " ('worldwide', 'NOUN'),\n",
       " ('on', 'ADP'),\n",
       " ('smartphones', 'NOUN'),\n",
       " ('since', 'ADP'),\n",
       " ('2011', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('on', 'ADP'),\n",
       " ('tablets', 'NOUN'),\n",
       " ('since', 'ADP'),\n",
       " ('2013', 'NOUN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vertibi_simple(\"Android has been the best-selling OS worldwide on smartphones since 2011 and on tablets since 2013.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Android is noun but incorrectly tagged as PRT, as emission prob for Android will be 0 and hence it will assign tag which has \n",
    "#o value. Let's see two more exaples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Google', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('Twitter', 'NOUN'),\n",
       " ('made', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('deal', 'NOUN'),\n",
       " ('in', 'ADP'),\n",
       " ('2015', 'NOUN'),\n",
       " ('that', 'ADP'),\n",
       " ('gave', 'VERB'),\n",
       " ('Google', 'NOUN'),\n",
       " ('access', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('Twitter', 'NOUN'),\n",
       " (\"'s\", 'PRT'),\n",
       " ('firehose', 'NOUN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vertibi_simple(\"Google and Twitter made a deal in 2015 that gave Google access to Twitter's firehose.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same problem for google and twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NASA', 'NOUN'),\n",
       " ('invited', 'NOUN'),\n",
       " ('social', 'ADJ'),\n",
       " ('media', 'NOUN'),\n",
       " ('users', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('experience', 'NOUN'),\n",
       " ('the', 'DET'),\n",
       " ('launch', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('ICESAT-2', 'NOUN'),\n",
       " ('Satellite', 'NOUN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vertibi_simple(\"NASA invited social media users to experience the launch of ICESAT-2 Satellite.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So we need to use other techni#sameques to rectify this issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve the problem of unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viterbi Heuristic\n",
    "def Viterbi(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "            state_probability = emission_p * transition_p    \n",
    "            p.append(state_probability)\n",
    "            \n",
    "        pmax = max(p)\n",
    "        # getting state for which probability is maximum\n",
    "        state_max = T[p.index(pmax)] \n",
    "        if max(p) == 0 :\n",
    "            state_max = T[3]\n",
    "        if word.isnumeric() & (word != '0'):\n",
    "            state_max = T[10]\n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Twitter', 'DET')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Viterbi(['Twitter'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating tagging accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the tagging accuracies of the modifications with the vanilla Viterbi algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List down cases which were incorrectly tagged by original POS tagger and got corrected by your modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
