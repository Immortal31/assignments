{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging using modified Viterbi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flowchart of the solution -\n",
    "\n",
    "<ul>\n",
    "    <li>EDA to understand training corpus.</li>\n",
    "    <li>Plain vanilla model building.</li>\n",
    "    <li>Test plain vanilla model on test set and understand the problem.</li>\n",
    "    <li>Refining viterbi model using other pos tagging technique</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "#Importing libraries\n",
    "import nltk, re, pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pprint, time\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the Treebank tagged sentences\n",
    "nltk_data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3718\n",
      "196\n"
     ]
    }
   ],
   "source": [
    "# Splitting into train and test\n",
    "random.seed(1234)\n",
    "train_set, test_set = train_test_split(nltk_data,test_size=0.05)\n",
    "\n",
    "print(len(train_set))\n",
    "print(len(test_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95891"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting list of tagged words\n",
    "train_tagged_words = [tup for sent in train_set for tup in sent]\n",
    "len(train_tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no of tagged words available in dataset - 95790"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "{'PRT', '.', 'ADV', 'NOUN', 'X', 'VERB', 'CONJ', 'ADP', 'NUM', 'ADJ', 'PRON', 'DET'}\n"
     ]
    }
   ],
   "source": [
    "#Let's see how many unique tags we have in our dataset\n",
    "tags = [tup[1]  for sen in nltk_data for tup in sen]\n",
    "print(len(set(tags)))\n",
    "print(set(tags))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100676\n"
     ]
    }
   ],
   "source": [
    "#we can see that universal dataset has only 12 tags\n",
    "#Let's see how many unique word dataset has\n",
    "voc = [tup[0]  for sen in nltk_data for tup in sen]\n",
    "print(len(voc))\n",
    "#total no. words present in dataset (including duplicate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = set(tags)\n",
    "voc = set(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PRT', '.', 'ADV', 'NOUN', 'X', 'VERB', 'CONJ', 'ADP', 'NUM', 'ADJ', 'PRON', 'DET'}\n"
     ]
    }
   ],
   "source": [
    "#print all the available tags\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this method we'll use to understand incorrect tags\n",
    "def plot_cnt_words(word):\n",
    "    l_words = []\n",
    "    for tag in tags:\n",
    "        c = 0\n",
    "        for w,t in train_tagged_words:\n",
    "            if (w==word)&(t==tag):\n",
    "                    c += 1\n",
    "        if c > 0:\n",
    "            l_words.append((tag,c))\n",
    "    print(l_words)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('PRON', 70)]\n"
     ]
    }
   ],
   "source": [
    "#demo\n",
    "plot_cnt_words(\"He\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the vanilla Viterbi based POS tagger\n",
    "Let's build HMM viterbi model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first step is to find emission and transition probablities\n",
    "t = len(tags)\n",
    "v = len(voc)\n",
    "w_given_t = np.zeros((t, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 12408)"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_given_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute word given tag: Emission Probability\n",
    "def word_given_tag(word, tag, train_bag = train_tagged_words):\n",
    "    tag_list = [pair for pair in train_bag if pair[1]==tag]\n",
    "    count_tag = len(tag_list)\n",
    "    w_given_tag_list = [pair[0] for pair in tag_list if pair[0]==word]\n",
    "    count_w_given_tag = len(w_given_tag_list)\n",
    "    \n",
    "    return (count_w_given_tag, count_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 12908)\n",
      "(53, 12908)\n",
      "(0, 27559) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's check w\n",
    "print(word_given_tag('do', 'VERB'))\n",
    "print(word_given_tag('does', 'VERB'))\n",
    "print(word_given_tag('flight', 'NOUN'), \"\\n\")\n",
    "#flight word is not present in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute tag given tag: tag2(t2) given tag1 (t1), i.e. Transition Probability\n",
    "\n",
    "def t2_given_t1(t2, t1, train_bag = train_tagged_words):\n",
    "    tags = [pair[1] for pair in train_bag]\n",
    "    count_t1 = len([t for t in tags if t==t1])\n",
    "    count_t2_t1 = 0\n",
    "    for index in range(len(tags)-1):\n",
    "        if tags[index]==t1 and tags[index+1] == t2:\n",
    "            count_t2_t1 += 1\n",
    "    return (count_t2_t1, count_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(92, 3014)\n"
     ]
    }
   ],
   "source": [
    "# examples\n",
    "print(t2_given_t1(t2='NOUN', t1='ADV'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating t x t transition matrix of tags\n",
    "# each column is t2, each row is t1\n",
    "# thus M(i, j) represents P(tj given ti)\n",
    "\n",
    "tags_matrix = np.zeros((len(tags), len(tags)), dtype='float32')\n",
    "for i, t1 in enumerate(list(tags)):\n",
    "    for j, t2 in enumerate(list(tags)): \n",
    "        tags_matrix[i, j] = t2_given_t1(t2, t1)[0]/t2_given_t1(t2, t1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.96206663e-03, 4.12034020e-02, 9.48332250e-03, 2.50490516e-01,\n",
       "        1.30804451e-02, 4.01569664e-01, 1.96206663e-03, 2.02746894e-02,\n",
       "        5.72269447e-02, 8.60039219e-02, 1.83126219e-02, 9.84303430e-02],\n",
       "       [2.24658521e-03, 9.25593078e-02, 5.23005016e-02, 2.23490298e-01,\n",
       "        2.72286125e-02, 8.88749138e-02, 5.73328547e-02, 9.20201316e-02,\n",
       "        8.14162493e-02, 4.35837545e-02, 6.50611073e-02, 1.73795834e-01],\n",
       "       [1.26078306e-02, 1.35368288e-01, 8.02919716e-02, 3.05242203e-02,\n",
       "        2.38885209e-02, 3.45719963e-01, 7.29927002e-03, 1.19442604e-01,\n",
       "        3.21831442e-02, 1.27737224e-01, 1.49303256e-02, 7.00066388e-02],\n",
       "       [4.41235155e-02, 2.39631340e-01, 1.71268918e-02, 2.64704823e-01,\n",
       "        2.92826295e-02, 1.46376863e-01, 4.27083708e-02, 1.76530346e-01,\n",
       "        9.28916130e-03, 1.20468810e-02, 4.78972401e-03, 1.33894552e-02],\n",
       "       [1.84235513e-01, 1.63342834e-01, 2.56410260e-02, 6.17283955e-02,\n",
       "        7.45489076e-02, 2.04653367e-01, 1.06046218e-02, 1.44666031e-01,\n",
       "        2.84900283e-03, 1.66191831e-02, 5.65052219e-02, 5.46058863e-02],\n",
       "       [3.07561196e-02, 3.47846299e-02, 8.19646716e-02, 1.11558720e-01,\n",
       "        2.17384562e-01, 1.69119924e-01, 5.11310808e-03, 9.06414613e-02,\n",
       "        2.37062294e-02, 6.49209768e-02, 3.56368162e-02, 1.34412766e-01],\n",
       "       [5.12581551e-03, 3.44827585e-02, 5.59179857e-02, 3.49953413e-01,\n",
       "        8.85368139e-03, 1.57036349e-01, 4.65983234e-04, 5.12581542e-02,\n",
       "        4.24044728e-02, 1.17893755e-01, 5.63839711e-02, 1.20223671e-01],\n",
       "       [1.38474652e-03, 4.02641669e-02, 1.28887938e-02, 3.21580738e-01,\n",
       "        3.53642963e-02, 7.98892230e-03, 7.45632744e-04, 1.68299954e-02,\n",
       "        6.26331493e-02, 1.06199406e-01, 6.93438426e-02, 3.24776322e-01],\n",
       "       [2.74093729e-02, 1.15237251e-01, 2.94724433e-03, 3.55437666e-01,\n",
       "        2.11022690e-01, 1.85676385e-02, 1.32625997e-02, 3.41880359e-02,\n",
       "        1.85086951e-01, 3.24196890e-02, 1.47362216e-03, 2.94724433e-03],\n",
       "       [1.03943245e-02, 6.35208711e-02, 4.45471052e-03, 7.01369405e-01,\n",
       "        2.12836172e-02, 1.20442174e-02, 1.66639164e-02, 7.75449574e-02,\n",
       "        2.04586703e-02, 6.64906800e-02, 6.59957121e-04, 5.11466758e-03],\n",
       "       [1.15118958e-02, 4.14428227e-02, 3.45356874e-02, 2.09132776e-01,\n",
       "        9.24788937e-02, 4.85801995e-01, 4.98848828e-03, 2.26400606e-02,\n",
       "        7.29086716e-03, 7.29086697e-02, 7.67459720e-03, 9.59324650e-03],\n",
       "       [2.40615977e-04, 1.72040425e-02, 1.21511072e-02, 6.39677584e-01,\n",
       "        4.59576510e-02, 3.91000956e-02, 3.60923965e-04, 9.62463953e-03,\n",
       "        2.22569779e-02, 2.04523578e-01, 3.60923959e-03, 5.29355137e-03]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the matrix to a df for better readability\n",
    "tags_df = pd.DataFrame(tags_matrix, columns = list(tags), index=list(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRT</th>\n",
       "      <th>.</th>\n",
       "      <th>ADV</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>X</th>\n",
       "      <th>VERB</th>\n",
       "      <th>CONJ</th>\n",
       "      <th>ADP</th>\n",
       "      <th>NUM</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>PRON</th>\n",
       "      <th>DET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PRT</th>\n",
       "      <td>0.001962</td>\n",
       "      <td>0.041203</td>\n",
       "      <td>0.009483</td>\n",
       "      <td>0.250491</td>\n",
       "      <td>0.013080</td>\n",
       "      <td>0.401570</td>\n",
       "      <td>0.001962</td>\n",
       "      <td>0.020275</td>\n",
       "      <td>0.057227</td>\n",
       "      <td>0.086004</td>\n",
       "      <td>0.018313</td>\n",
       "      <td>0.098430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.002247</td>\n",
       "      <td>0.092559</td>\n",
       "      <td>0.052301</td>\n",
       "      <td>0.223490</td>\n",
       "      <td>0.027229</td>\n",
       "      <td>0.088875</td>\n",
       "      <td>0.057333</td>\n",
       "      <td>0.092020</td>\n",
       "      <td>0.081416</td>\n",
       "      <td>0.043584</td>\n",
       "      <td>0.065061</td>\n",
       "      <td>0.173796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>0.012608</td>\n",
       "      <td>0.135368</td>\n",
       "      <td>0.080292</td>\n",
       "      <td>0.030524</td>\n",
       "      <td>0.023889</td>\n",
       "      <td>0.345720</td>\n",
       "      <td>0.007299</td>\n",
       "      <td>0.119443</td>\n",
       "      <td>0.032183</td>\n",
       "      <td>0.127737</td>\n",
       "      <td>0.014930</td>\n",
       "      <td>0.070007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>0.044124</td>\n",
       "      <td>0.239631</td>\n",
       "      <td>0.017127</td>\n",
       "      <td>0.264705</td>\n",
       "      <td>0.029283</td>\n",
       "      <td>0.146377</td>\n",
       "      <td>0.042708</td>\n",
       "      <td>0.176530</td>\n",
       "      <td>0.009289</td>\n",
       "      <td>0.012047</td>\n",
       "      <td>0.004790</td>\n",
       "      <td>0.013389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>0.184236</td>\n",
       "      <td>0.163343</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.061728</td>\n",
       "      <td>0.074549</td>\n",
       "      <td>0.204653</td>\n",
       "      <td>0.010605</td>\n",
       "      <td>0.144666</td>\n",
       "      <td>0.002849</td>\n",
       "      <td>0.016619</td>\n",
       "      <td>0.056505</td>\n",
       "      <td>0.054606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>0.030756</td>\n",
       "      <td>0.034785</td>\n",
       "      <td>0.081965</td>\n",
       "      <td>0.111559</td>\n",
       "      <td>0.217385</td>\n",
       "      <td>0.169120</td>\n",
       "      <td>0.005113</td>\n",
       "      <td>0.090641</td>\n",
       "      <td>0.023706</td>\n",
       "      <td>0.064921</td>\n",
       "      <td>0.035637</td>\n",
       "      <td>0.134413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONJ</th>\n",
       "      <td>0.005126</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.055918</td>\n",
       "      <td>0.349953</td>\n",
       "      <td>0.008854</td>\n",
       "      <td>0.157036</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.051258</td>\n",
       "      <td>0.042404</td>\n",
       "      <td>0.117894</td>\n",
       "      <td>0.056384</td>\n",
       "      <td>0.120224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADP</th>\n",
       "      <td>0.001385</td>\n",
       "      <td>0.040264</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.321581</td>\n",
       "      <td>0.035364</td>\n",
       "      <td>0.007989</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>0.016830</td>\n",
       "      <td>0.062633</td>\n",
       "      <td>0.106199</td>\n",
       "      <td>0.069344</td>\n",
       "      <td>0.324776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>0.027409</td>\n",
       "      <td>0.115237</td>\n",
       "      <td>0.002947</td>\n",
       "      <td>0.355438</td>\n",
       "      <td>0.211023</td>\n",
       "      <td>0.018568</td>\n",
       "      <td>0.013263</td>\n",
       "      <td>0.034188</td>\n",
       "      <td>0.185087</td>\n",
       "      <td>0.032420</td>\n",
       "      <td>0.001474</td>\n",
       "      <td>0.002947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADJ</th>\n",
       "      <td>0.010394</td>\n",
       "      <td>0.063521</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>0.701369</td>\n",
       "      <td>0.021284</td>\n",
       "      <td>0.012044</td>\n",
       "      <td>0.016664</td>\n",
       "      <td>0.077545</td>\n",
       "      <td>0.020459</td>\n",
       "      <td>0.066491</td>\n",
       "      <td>0.000660</td>\n",
       "      <td>0.005115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRON</th>\n",
       "      <td>0.011512</td>\n",
       "      <td>0.041443</td>\n",
       "      <td>0.034536</td>\n",
       "      <td>0.209133</td>\n",
       "      <td>0.092479</td>\n",
       "      <td>0.485802</td>\n",
       "      <td>0.004988</td>\n",
       "      <td>0.022640</td>\n",
       "      <td>0.007291</td>\n",
       "      <td>0.072909</td>\n",
       "      <td>0.007675</td>\n",
       "      <td>0.009593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DET</th>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.017204</td>\n",
       "      <td>0.012151</td>\n",
       "      <td>0.639678</td>\n",
       "      <td>0.045958</td>\n",
       "      <td>0.039100</td>\n",
       "      <td>0.000361</td>\n",
       "      <td>0.009625</td>\n",
       "      <td>0.022257</td>\n",
       "      <td>0.204524</td>\n",
       "      <td>0.003609</td>\n",
       "      <td>0.005294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           PRT         .       ADV      NOUN         X      VERB      CONJ  \\\n",
       "PRT   0.001962  0.041203  0.009483  0.250491  0.013080  0.401570  0.001962   \n",
       ".     0.002247  0.092559  0.052301  0.223490  0.027229  0.088875  0.057333   \n",
       "ADV   0.012608  0.135368  0.080292  0.030524  0.023889  0.345720  0.007299   \n",
       "NOUN  0.044124  0.239631  0.017127  0.264705  0.029283  0.146377  0.042708   \n",
       "X     0.184236  0.163343  0.025641  0.061728  0.074549  0.204653  0.010605   \n",
       "VERB  0.030756  0.034785  0.081965  0.111559  0.217385  0.169120  0.005113   \n",
       "CONJ  0.005126  0.034483  0.055918  0.349953  0.008854  0.157036  0.000466   \n",
       "ADP   0.001385  0.040264  0.012889  0.321581  0.035364  0.007989  0.000746   \n",
       "NUM   0.027409  0.115237  0.002947  0.355438  0.211023  0.018568  0.013263   \n",
       "ADJ   0.010394  0.063521  0.004455  0.701369  0.021284  0.012044  0.016664   \n",
       "PRON  0.011512  0.041443  0.034536  0.209133  0.092479  0.485802  0.004988   \n",
       "DET   0.000241  0.017204  0.012151  0.639678  0.045958  0.039100  0.000361   \n",
       "\n",
       "           ADP       NUM       ADJ      PRON       DET  \n",
       "PRT   0.020275  0.057227  0.086004  0.018313  0.098430  \n",
       ".     0.092020  0.081416  0.043584  0.065061  0.173796  \n",
       "ADV   0.119443  0.032183  0.127737  0.014930  0.070007  \n",
       "NOUN  0.176530  0.009289  0.012047  0.004790  0.013389  \n",
       "X     0.144666  0.002849  0.016619  0.056505  0.054606  \n",
       "VERB  0.090641  0.023706  0.064921  0.035637  0.134413  \n",
       "CONJ  0.051258  0.042404  0.117894  0.056384  0.120224  \n",
       "ADP   0.016830  0.062633  0.106199  0.069344  0.324776  \n",
       "NUM   0.034188  0.185087  0.032420  0.001474  0.002947  \n",
       "ADJ   0.077545  0.020459  0.066491  0.000660  0.005115  \n",
       "PRON  0.022640  0.007291  0.072909  0.007675  0.009593  \n",
       "DET   0.009625  0.022257  0.204524  0.003609  0.005294  "
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this dataframe is usefull to calculate tag probablities\n",
    "tags_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viterbi Algorithm - Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95891"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viterbi Heuristic\n",
    "def Viterbi(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "            state_probability = emission_p * transition_p    \n",
    "            p.append(state_probability)\n",
    "            \n",
    "        pmax = max(p)\n",
    "        # getting state for which probability is maximum\n",
    "        state_max = T[p.index(pmax)] \n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluating on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Mr.', 'NOUN'),\n",
       "  ('Yamamoto', 'NOUN'),\n",
       "  ('insisted', 'VERB'),\n",
       "  ('that', 'ADP'),\n",
       "  ('headquarters', 'NOUN'),\n",
       "  ('had', 'VERB'),\n",
       "  (\"n't\", 'ADV'),\n",
       "  ('approved', 'VERB'),\n",
       "  ('the', 'DET'),\n",
       "  ('bids', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('and', 'CONJ'),\n",
       "  ('that', 'ADP'),\n",
       "  ('he', 'PRON'),\n",
       "  ('did', 'VERB'),\n",
       "  (\"n't\", 'ADV'),\n",
       "  ('know', 'VERB'),\n",
       "  ('about', 'ADP'),\n",
       "  ('most', 'ADJ'),\n",
       "  ('of', 'ADP'),\n",
       "  ('the', 'DET'),\n",
       "  ('cases', 'NOUN'),\n",
       "  ('until', 'ADP'),\n",
       "  ('Wednesday', 'NOUN'),\n",
       "  ('.', '.')],\n",
       " [('Mr.', 'NOUN'),\n",
       "  ('Bernstein', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('a', 'DET'),\n",
       "  ('tall', 'ADJ'),\n",
       "  (',', '.'),\n",
       "  ('energetic', 'ADJ'),\n",
       "  ('man', 'NOUN'),\n",
       "  ('who', 'PRON'),\n",
       "  ('*T*-39', 'X'),\n",
       "  ('is', 'VERB'),\n",
       "  ('widely', 'ADV'),\n",
       "  ('respected', 'VERB'),\n",
       "  ('as', 'ADP'),\n",
       "  ('a', 'DET'),\n",
       "  ('publishing', 'NOUN'),\n",
       "  ('executive', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('has', 'VERB'),\n",
       "  ('spent', 'VERB'),\n",
       "  ('much', 'ADJ'),\n",
       "  ('of', 'ADP'),\n",
       "  ('his', 'PRON'),\n",
       "  ('time', 'NOUN'),\n",
       "  ('in', 'ADP'),\n",
       "  ('recent', 'ADJ'),\n",
       "  ('years', 'NOUN'),\n",
       "  ('on', 'ADP'),\n",
       "  ('human', 'ADJ'),\n",
       "  ('rights', 'NOUN'),\n",
       "  ('issues', 'NOUN'),\n",
       "  ('.', '.')],\n",
       " [('And', 'CONJ'),\n",
       "  ('while', 'ADP'),\n",
       "  ('there', 'DET'),\n",
       "  ('was', 'VERB'),\n",
       "  ('no', 'DET'),\n",
       "  ('profit', 'NOUN'),\n",
       "  ('*ICH*-1', 'X'),\n",
       "  ('this', 'DET'),\n",
       "  ('year', 'NOUN'),\n",
       "  ('from', 'ADP'),\n",
       "  ('discontinued', 'VERB'),\n",
       "  ('operations', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('last', 'ADJ'),\n",
       "  ('year', 'NOUN'),\n",
       "  ('they', 'PRON'),\n",
       "  ('contributed', 'VERB'),\n",
       "  ('#', '.'),\n",
       "  ('34', 'NUM'),\n",
       "  ('million', 'NUM'),\n",
       "  ('*U*', 'X'),\n",
       "  (',', '.'),\n",
       "  ('before', 'ADP'),\n",
       "  ('tax', 'NOUN'),\n",
       "  ('.', '.')],\n",
       " [('``', '.'),\n",
       "  ('This', 'DET'),\n",
       "  ('form', 'NOUN'),\n",
       "  ('forces', 'VERB'),\n",
       "  ('a', 'DET'),\n",
       "  ('lawyer', 'NOUN'),\n",
       "  ('to', 'PRT'),\n",
       "  ('become', 'VERB'),\n",
       "  (',', '.'),\n",
       "  ('in', 'ADP'),\n",
       "  ('effect', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('a', 'DET'),\n",
       "  ('witness', 'NOUN'),\n",
       "  ('against', 'ADP'),\n",
       "  ('his', 'PRON'),\n",
       "  ('client', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  (\"''\", '.'),\n",
       "  ('said', 'VERB'),\n",
       "  ('*T*-1', 'X'),\n",
       "  ('Neal', 'NOUN'),\n",
       "  ('R.', 'NOUN'),\n",
       "  ('Sonnett', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('president', 'NOUN'),\n",
       "  ('of', 'ADP'),\n",
       "  ('the', 'DET'),\n",
       "  ('National', 'NOUN'),\n",
       "  ('Association', 'NOUN'),\n",
       "  ('of', 'ADP'),\n",
       "  ('Criminal', 'NOUN'),\n",
       "  ('Defense', 'NOUN'),\n",
       "  ('Lawyers', 'NOUN'),\n",
       "  ('.', '.')],\n",
       " [('``', '.'),\n",
       "  ('Strategic', 'ADJ'),\n",
       "  ('objectives', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('not', 'ADV'),\n",
       "  ('financial', 'ADJ'),\n",
       "  ('return', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('drive', 'VERB'),\n",
       "  ('many', 'ADJ'),\n",
       "  ('of', 'ADP'),\n",
       "  ('the', 'DET'),\n",
       "  ('deals', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  (\"''\", '.'),\n",
       "  ('says', 'VERB'),\n",
       "  ('*T*-1', 'X'),\n",
       "  ('a', 'DET'),\n",
       "  ('Venture', 'NOUN'),\n",
       "  ('Economics', 'NOUN'),\n",
       "  ('spokesman', 'NOUN'),\n",
       "  ('.', '.')]]"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Running on entire test dataset would take more than 3-4hrs. \n",
    "# Let's test our Viterbi algorithm on a few sample sentences of test dataset\n",
    "\n",
    "random.seed(1234)\n",
    "\n",
    "# choose random 5 sents\n",
    "rndom = [random.randint(1,len(test_set)) for x in range(5)]\n",
    "\n",
    "# list of sents\n",
    "test_run = [test_set[i] for i in rndom]\n",
    "\n",
    "# list of tagged words\n",
    "test_run_base = [tup for sent in test_run for tup in sent]\n",
    "\n",
    "# list of untagged words\n",
    "test_tagged_words = [tup[0] for sent in test_run for tup in sent]\n",
    "test_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagging the test sentences\n",
    "start = time.time()\n",
    "tagged_seq = Viterbi(test_tagged_words)\n",
    "end = time.time()\n",
    "difference = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in seconds:  28.722251176834106\n",
      "[('Mr.', 'NOUN'), ('Yamamoto', 'NOUN'), ('insisted', 'VERB'), ('that', 'ADP'), ('headquarters', 'NOUN'), ('had', 'VERB'), (\"n't\", 'ADV'), ('approved', 'VERB'), ('the', 'DET'), ('bids', 'NOUN'), (',', '.'), ('and', 'CONJ'), ('that', 'DET'), ('he', 'PRON'), ('did', 'VERB'), (\"n't\", 'ADV'), ('know', 'VERB'), ('about', 'ADP'), ('most', 'ADJ'), ('of', 'ADP'), ('the', 'DET'), ('cases', 'NOUN'), ('until', 'ADP'), ('Wednesday', 'NOUN'), ('.', '.'), ('Mr.', 'NOUN'), ('Bernstein', 'NOUN'), (',', '.'), ('a', 'DET'), ('tall', 'PRT'), (',', '.'), ('energetic', 'ADJ'), ('man', 'NOUN'), ('who', 'PRON'), ('*T*-39', 'X'), ('is', 'VERB'), ('widely', 'ADV'), ('respected', 'VERB'), ('as', 'ADP'), ('a', 'DET'), ('publishing', 'NOUN'), ('executive', 'NOUN'), (',', '.'), ('has', 'VERB'), ('spent', 'VERB'), ('much', 'ADV'), ('of', 'ADP'), ('his', 'PRON'), ('time', 'NOUN'), ('in', 'ADP'), ('recent', 'ADJ'), ('years', 'NOUN'), ('on', 'ADP'), ('human', 'ADJ'), ('rights', 'NOUN'), ('issues', 'NOUN'), ('.', '.'), ('And', 'CONJ'), ('while', 'ADP'), ('there', 'DET'), ('was', 'VERB'), ('no', 'DET'), ('profit', 'NOUN'), ('*ICH*-1', 'X'), ('this', 'DET'), ('year', 'NOUN'), ('from', 'ADP'), ('discontinued', 'VERB'), ('operations', 'NOUN'), (',', '.'), ('last', 'ADJ'), ('year', 'NOUN'), ('they', 'PRON'), ('contributed', 'VERB'), ('#', '.'), ('34', 'NUM'), ('million', 'NUM'), ('*U*', 'X'), (',', '.'), ('before', 'ADP'), ('tax', 'NOUN'), ('.', '.'), ('``', '.'), ('This', 'DET'), ('form', 'NOUN'), ('forces', 'NOUN'), ('a', 'DET'), ('lawyer', 'NOUN'), ('to', 'PRT'), ('become', 'VERB'), (',', '.'), ('in', 'ADP'), ('effect', 'NOUN'), (',', '.'), ('a', 'DET'), ('witness', 'PRT'), ('against', 'ADP'), ('his', 'PRON'), ('client', 'NOUN'), (',', '.'), (\"''\", '.'), ('said', 'VERB'), ('*T*-1', 'X'), ('Neal', 'NOUN'), ('R.', 'NOUN'), ('Sonnett', 'NOUN'), (',', '.'), ('president', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('National', 'NOUN'), ('Association', 'NOUN'), ('of', 'ADP'), ('Criminal', 'ADJ'), ('Defense', 'NOUN'), ('Lawyers', 'NOUN'), ('.', '.'), ('``', '.'), ('Strategic', 'PRT'), ('objectives', 'NOUN'), (',', '.'), ('not', 'ADV'), ('financial', 'ADJ'), ('return', 'NOUN'), (',', '.'), ('drive', 'VERB'), ('many', 'ADJ'), ('of', 'ADP'), ('the', 'DET'), ('deals', 'NOUN'), (',', '.'), (\"''\", '.'), ('says', 'VERB'), ('*T*-1', 'X'), ('a', 'DET'), ('Venture', 'NOUN'), ('Economics', 'NOUN'), ('spokesman', 'NOUN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Time taken in seconds: \", difference)\n",
    "print(tagged_seq)\n",
    "#print(test_run_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "check = [i for i, j in zip(tagged_seq, test_run_base) if i == j] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = len(check)/len(tagged_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9496402877697842"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy\n",
    "#92#94.62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_tagged_cases = [[test_run_base[i-1],j] for i, j in enumerate(zip(tagged_seq, test_run_base)) if j[0]!=j[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('and', 'CONJ'), (('that', 'DET'), ('that', 'ADP'))],\n",
       " [('a', 'DET'), (('tall', 'PRT'), ('tall', 'ADJ'))],\n",
       " [('spent', 'VERB'), (('much', 'ADV'), ('much', 'ADJ'))],\n",
       " [('form', 'NOUN'), (('forces', 'NOUN'), ('forces', 'VERB'))],\n",
       " [('a', 'DET'), (('witness', 'PRT'), ('witness', 'NOUN'))],\n",
       " [('of', 'ADP'), (('Criminal', 'ADJ'), ('Criminal', 'NOUN'))],\n",
       " [('``', '.'), (('Strategic', 'PRT'), ('Strategic', 'ADJ'))]]"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_tagged_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's test this model on test sentences which contains words which are not present in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing\n",
    "\n",
    "def test_vertibi_simple(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    tagged_seq = Viterbi(words)\n",
    "    return tagged_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Android', 'PRT'),\n",
       " ('has', 'VERB'),\n",
       " ('been', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('best-selling', 'ADJ'),\n",
       " ('OS', 'PRT'),\n",
       " ('worldwide', 'PRT'),\n",
       " ('on', 'ADP'),\n",
       " ('smartphones', 'PRT'),\n",
       " ('since', 'ADP'),\n",
       " ('2011', 'PRT'),\n",
       " ('and', 'CONJ'),\n",
       " ('on', 'ADP'),\n",
       " ('tablets', 'NOUN'),\n",
       " ('since', 'ADP'),\n",
       " ('2013', 'PRT'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vertibi_simple(\"Android has been the best-selling OS worldwide on smartphones since 2011 and on tablets since 2013.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Android is noun but incorrectly tagged as PRT, as emission prob for Android will be 0 and hence it will assign tag which has \n",
    "#o value. Let's see two more exaples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Google', 'PRT'),\n",
       " ('and', 'CONJ'),\n",
       " ('Twitter', 'PRT'),\n",
       " ('made', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('deal', 'NOUN'),\n",
       " ('in', 'ADP'),\n",
       " ('2015', 'PRT'),\n",
       " ('that', 'DET'),\n",
       " ('gave', 'VERB'),\n",
       " ('Google', 'PRT'),\n",
       " ('access', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('Twitter', 'PRT'),\n",
       " (\"'s\", 'VERB'),\n",
       " ('firehose', 'PRT'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vertibi_simple(\"Google and Twitter made a deal in 2015 that gave Google access to Twitter's firehose.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same problem for google and twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NASA', 'PRT'),\n",
       " ('invited', 'PRT'),\n",
       " ('social', 'ADJ'),\n",
       " ('media', 'NOUN'),\n",
       " ('users', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('experience', 'NOUN'),\n",
       " ('the', 'DET'),\n",
       " ('launch', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('ICESAT-2', 'PRT'),\n",
       " ('Satellite', 'PRT'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vertibi_simple(\"NASA invited social media users to experience the launch of ICESAT-2 Satellite.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So we need to use other techni#sameques to rectify this issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve the problem of unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viterbi Heuristic\n",
    "def Viterbi(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "            state_probability = emission_p * transition_p    \n",
    "            p.append(state_probability)\n",
    "            \n",
    "        pmax = max(p)\n",
    "        # getting state for which probability is maximum\n",
    "        state_max = T[p.index(pmax)] \n",
    "        if max(p) == 0 :\n",
    "            state_max = T[3]\n",
    "        if word.isnumeric() & (word != '0'):\n",
    "            state_max = T[10]\n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating tagging accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the tagging accuracies of the modifications with the vanilla Viterbi algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List down cases which were incorrectly tagged by original POS tagger and got corrected by your modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
